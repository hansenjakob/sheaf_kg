{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pykeen\n",
    "import torch\n",
    "\n",
    "from pykeen.pipeline import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'FB15k-237'\n",
    "num_epochs = 1000\n",
    "embedding_dim = 64\n",
    "num_sections = 25\n",
    "random_seed=1235\n",
    "loss = 'CrossEntropyLoss'\n",
    "training_loop = 'lcwa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import itertools\n",
    "from typing import Optional\n",
    "\n",
    "from pykeen.models import StructuredEmbedding\n",
    "from pykeen.models.base import _OldAbstractModel\n",
    "from pykeen.nn import Embedding\n",
    "from pykeen.losses import Loss\n",
    "from pykeen.nn.init import xavier_uniform_\n",
    "from pykeen.regularizers import Regularizer\n",
    "from pykeen.triples import TriplesFactory\n",
    "from pykeen.typing import DeviceHint\n",
    "from pykeen.utils import compose\n",
    "\n",
    "from torch.nn import functional\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch import nn\n",
    "\n",
    "import geotorch\n",
    "\n",
    "class ModifiedSE(_OldAbstractModel):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        triples_factory: TriplesFactory,\n",
    "        embedding_dim: int = 20,\n",
    "        scoring_fct_norm: int = 2,\n",
    "        num_sections: int = 10,\n",
    "        loss: Optional[Loss] = None,\n",
    "        preferred_device: DeviceHint = None,\n",
    "        random_seed: Optional[int] = None,\n",
    "        regularizer: Optional[Regularizer] = None,\n",
    "    ) -> None:\n",
    "        r\"\"\"Initialize SE.\n",
    "\n",
    "        :param embedding_dim: The entity embedding dimension $d$. Is usually $d \\in [50, 300]$.\n",
    "        :param scoring_fct_norm: The $l_p$ norm. Usually 1 for SE.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            triples_factory=triples_factory,\n",
    "            loss=loss,\n",
    "            preferred_device=preferred_device,\n",
    "            random_seed=random_seed,\n",
    "            regularizer=regularizer,\n",
    "        )\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_sections = num_sections\n",
    "        self.scoring_fct_norm = scoring_fct_norm\n",
    "        \n",
    "        esize = (triples_factory.num_entities, num_sections, embedding_dim)\n",
    "        self.ent_embeddings = Parameter(nn.init.xavier_uniform_(torch.empty(esize, device=preferred_device, dtype=torch.float32)),requires_grad=True)\n",
    "        \n",
    "        tsize = (triples_factory.num_relations, embedding_dim, embedding_dim)\n",
    "        self.left_embeddings = Parameter(nn.init.xavier_uniform_(torch.empty(tsize, device=preferred_device, dtype=torch.float32)),requires_grad=True)\n",
    "        self.right_embeddings = Parameter(nn.init.xavier_uniform_(torch.empty(tsize, device=preferred_device, requires_grad=True, dtype=torch.float32)),requires_grad=True)\n",
    "        \n",
    "    def _reset_parameters_(self):  # noqa: D102\n",
    "        self.ent_embeddings = nn.init.xavier_uniform_(self.ent_embeddings)\n",
    "        self.left_embeddings = nn.init.xavier_uniform_(self.left_embeddings)\n",
    "        self.right_embeddings = nn.init.xavier_uniform_(self.right_embeddings)\n",
    "        \n",
    "        \n",
    "    def score_hrt(self, hrt_batch: torch.LongTensor) -> torch.FloatTensor:  # noqa: D102\n",
    "        # Get embeddings\n",
    "        h = torch.index_select(self.ent_embeddings, 0, hrt_batch[:, 0]).view(-1, self.embedding_dim, self.num_sections)\n",
    "        rel_h = torch.index_select(self.left_embeddings, 0, hrt_batch[:, 1])\n",
    "        rel_t = torch.index_select(self.right_embeddings, 0, hrt_batch[:, 1])\n",
    "        t = torch.index_select(self.ent_embeddings, 0, hrt_batch[:, 2]).view(-1, self.embedding_dim, self.num_sections)\n",
    "        \n",
    "        # Project entities\n",
    "        proj_h = rel_h @ h\n",
    "        proj_t = rel_t @ t\n",
    "        scores = -torch.norm(proj_h - proj_t, dim=(1,2), p=self.scoring_fct_norm)\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def score_t(self, hr_batch: torch.LongTensor, slice_size: int = None) -> torch.FloatTensor:  # noqa: D102\n",
    "        # Get embeddings\n",
    "        h = torch.index_select(self.ent_embeddings, 0, hr_batch[:, 0]).view(-1, self.embedding_dim, self.num_sections)\n",
    "        rel_h = torch.index_select(self.left_embeddings, 0, hr_batch[:, 1])\n",
    "        rel_t = torch.index_select(self.right_embeddings, 0, hr_batch[:, 1])\n",
    "        rel_t = rel_t.view(-1, 1, self.embedding_dim, self.embedding_dim)\n",
    "        t_all = self.ent_embeddings.view(1, -1, self.embedding_dim, self.num_sections)\n",
    "\n",
    "        if slice_size is not None:\n",
    "            raise ValueError('Not implemented')\n",
    "\n",
    "        else:\n",
    "            # Project entities\n",
    "            proj_h = rel_h @ h\n",
    "            proj_t = rel_t @ t_all\n",
    "\n",
    "        scores = -torch.norm(proj_h[:, None, :, :] - proj_t[:, :, :, :], dim=(-1,-2), p=self.scoring_fct_norm)\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def score_h(self, rt_batch: torch.LongTensor, slice_size: int = None) -> torch.FloatTensor:  # noqa: D102\n",
    "        # Get embeddings\n",
    "        h_all = self.ent_embeddings.view(1, -1, self.embedding_dim, self.num_sections)\n",
    "        rel_h = torch.index_select(self.left_embeddings, 0, rt_batch[:, 0])\n",
    "        rel_h = rel_h.view(-1, 1, self.embedding_dim, self.embedding_dim)\n",
    "        rel_t = torch.index_select(self.right_embeddings, 0, rt_batch[:, 0])\n",
    "        t = torch.index_select(self.ent_embeddings, 0, rt_batch[:, 1]).view(-1, self.embedding_dim, self.num_sections)\n",
    "\n",
    "        if slice_size is not None:\n",
    "            raise ValueError('Not implemented')\n",
    "        else:\n",
    "            # Project entities\n",
    "            proj_h = rel_h @ h_all\n",
    "            proj_t = rel_t @ t\n",
    "\n",
    "        scores = -torch.norm(proj_h[:, :, :, :] - proj_t[:, None, :, :], dim=(-1,-2), p=self.scoring_fct_norm)\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result2 = pipeline(\n",
    "    model=ModifiedSE,\n",
    "    dataset=dataset,\n",
    "    random_seed=random_seed,\n",
    "    device='gpu',\n",
    "    training_kwargs=dict(num_epochs=num_epochs),\n",
    "    evaluation_kwargs=dict(),\n",
    "    model_kwargs=dict(embedding_dim=embedding_dim, num_sections=num_sections),\n",
    "    loss=loss,\n",
    "    training_loop=training_loop,\n",
    "    loss_kwargs=dict()\n",
    ")\n",
    "model2 = result2.model\n",
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2.plot_losses()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_models = ['StructuredEmbedding','TransE','RotatE']\n",
    "comp_results = []\n",
    "for comp_model in comp_models:\n",
    "    print('Running {}'.format(comp_model))\n",
    "    result = pipeline(\n",
    "        dataset=dataset,\n",
    "        model=comp_model,\n",
    "        random_seed=random_seed,\n",
    "        device='gpu',\n",
    "        training_kwargs=dict(num_epochs=num_epochs),\n",
    "        model_kwargs=dict(embedding_dim=embedding_dim),\n",
    "        loss=loss,\n",
    "        training_loop='slcwa',\n",
    "        loss_kwargs=dict()\n",
    "    )\n",
    "    comp_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(result2.losses)),result2.losses,label='Sheaf SE')\n",
    "for i in range(len(comp_models)):\n",
    "    comp_model = comp_models[i]\n",
    "    comp_result = comp_results[i]\n",
    "    plt.plot(np.arange(len(comp_result.losses)),comp_result.losses,label=comp_model)\n",
    "plt.ylabel(str(result.model.loss).replace('()',''))\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = result2.metric_results.to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compto = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df['diff'] = res_df.Value - comp_results[compto].metric_results.to_df().Value\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestr = time.strftime(\"%Y%m%d-%H%M\")\n",
    "savename = 'SheafE_multisection_{}_sections_{}epochs_{}dim_{}loss_{}seed_{}'.format(num_sections,num_epochs,embedding_dim,loss,random_seed,timestr)\n",
    "saveloc = os.path.join('/home/gebhart/projects/sheaf_kg/data',dataset,savename)\n",
    "res_df.to_csv(saveloc+'.csv')\n",
    "result2.save_to_directory(saveloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
