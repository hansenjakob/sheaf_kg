{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "os.chdir('/home/gebhart/projects/sheaf_kg')\n",
    "import sheaf_kg.harmonic_extension as harmonic_extension\n",
    "# from sheaf_kg.train_sheafE_nonconstant_diag import ModifiedSE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pykeen\n",
    "import torch\n",
    "from pykeen.pipeline import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'FB15k'\n",
    "num_test = 1000\n",
    "path_len = 2\n",
    "model_name = 'StructuredEmbedding_1000epochs_64dim_SoftplusLossloss_42seed_20210128-1346'\n",
    "save_loc = '/home/gebhart/projects/sheaf_kg/data/{}/{}/trained_model.pkl'.format(dataset,model_name)\n",
    "model = torch.load(save_loc).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds = pykeen.datasets.get_dataset(dataset=dataset)\n",
    "\n",
    "training = ds.training.mapped_triples\n",
    "testing = ds.testing.mapped_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_hop_dataset(training, test_size, path_length):\n",
    "    ''' Function to random walk on knowledge graph triplets to generate\n",
    "    multi-hop testing dataset. This function assumes `training` is of the \n",
    "    form provided by pykeen. That is, a tensor of size (d x 3) where d \n",
    "    is the number of triplets, column 1 is the head entity id, column 2 \n",
    "    is the relation id, and column 3 is the tail entity id. \n",
    "    \n",
    "    There is likely a much faster way to generate this dataset with better \n",
    "    preprocessing and a better representation for the triplet graph. But \n",
    "    this should do for now.\n",
    "    '''\n",
    "    head_ents = torch.unique(training[:,0])\n",
    "    tail_ents = torch.unique(training[:,2])\n",
    "    unq_ents = torch.unique(torch.cat([head_ents,tail_ents]))\n",
    "    random_start_idxs = torch.randint(unq_ents.shape[0], (test_size,))\n",
    "    \n",
    "    path_ents = torch.zeros((test_size,path_length), dtype=torch.int64) # track nodes along random walk\n",
    "    comp_relations = torch.zeros((test_size,path_length), dtype=torch.int64) # track the relations crossed along each walk\n",
    "    inv_relations = torch.zeros((test_size,path_length), dtype=torch.int64) # track whether the crossed relations are inverted\n",
    "    for pidx in range(test_size):\n",
    "        random_start_idx = random_start_idxs[pidx]\n",
    "        ent = unq_ents[random_start_idx]\n",
    "        path_ents[pidx,0] = ent\n",
    "        for step in range(path_length):\n",
    "            head_instances = training[training[:,0] == ent]\n",
    "            tail_instances = training[training[:,2] == ent]\n",
    "            instances = torch.cat([head_instances, tail_instances])\n",
    "            random_step_idx = torch.randint(instances.shape[0], (1,))\n",
    "            step_edge = instances[random_step_idx[0]]\n",
    "            if step_edge[0] == ent:\n",
    "                # forward relation\n",
    "                inv_relations[pidx,step] = 1\n",
    "                ent = step_edge[2]\n",
    "            else:\n",
    "                # inverse relation\n",
    "                inv_relations[pidx,step] = -1\n",
    "                ent = step_edge[0]\n",
    "            path_ents[pidx,step] = ent\n",
    "            comp_relations[pidx,step] = step_edge[1]\n",
    "    return path_ents, comp_relations, inv_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ents, comp_relations, inv_relations = create_multi_hop_dataset(training, num_test, path_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5000, 2]), torch.Size([5000, 2]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_ents.shape, comp_relations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredEmbedding(\n",
       "  (loss): SoftplusLoss(\n",
       "    (softplus): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (regularizer): NoRegularizer()\n",
       "  (entity_embeddings): Embedding(\n",
       "    (_embeddings): Embedding(14951, 64)\n",
       "  )\n",
       "  (left_relation_embeddings): Embedding(\n",
       "    (_embeddings): Embedding(1345, 4096)\n",
       "  )\n",
       "  (right_relation_embeddings): Embedding(\n",
       "    (_embeddings): Embedding(1345, 4096)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_at = [1,3,5,10]\n",
    "results = np.zeros((path_ents.shape[0],len(hits_at)))\n",
    "for i in range(path_ents.shape[0]):\n",
    "    path_ent = path_ents[i,:]\n",
    "    comp_rel = comp_relations[i,:]\n",
    "    inverses = inv_relations[i,:]\n",
    "\n",
    "    source_ents = path_ent[:-1]\n",
    "\n",
    "    # create edge indices as required by harmonic_extension.py, these are linear chains which index into path_ent\n",
    "    edge_indices = np.concatenate([np.arange(0,path_ent.shape[0])[:,np.newaxis].T, np.arange(1,path_ent.shape[0]+1)[:,np.newaxis].T], axis=0)\n",
    "\n",
    "    source_embeddings = model.entity_embeddings(indices=source_ents).view(-1, model.embedding_dim).detach().numpy()\n",
    "    target_embeddings = model.entity_embeddings(indices=None).view(-1, model.embedding_dim).detach().numpy()\n",
    "\n",
    "    left_restrictions = model.left_relation_embeddings(indices=comp_rel).view(-1, model.embedding_dim, model.embedding_dim).detach().numpy()\n",
    "    right_restrictions = model.right_relation_embeddings(indices=comp_rel).view(-1, model.embedding_dim, model.embedding_dim).detach().numpy()\n",
    "\n",
    "    restrictions = np.empty((comp_rel.shape[0], 2, left_restrictions.shape[2], left_restrictions.shape[1]))\n",
    "    for j in range(inverses.shape[0]):\n",
    "        if inverses[j] == -1:\n",
    "            restrictions[j,0] = right_restrictions[j]\n",
    "            restrictions[j,1] = left_restrictions[j]\n",
    "        else:\n",
    "            restrictions[j,0] = left_restrictions[j]\n",
    "            restrictions[j,1] = right_restrictions[j]\n",
    "#         restrictions[j,0] = left_restrictions[j]\n",
    "#         restrictions[j,1] = right_restrictions[j]\n",
    "\n",
    "    L = harmonic_extension.Laplacian(edge_indices, restrictions)\n",
    "    \n",
    "    source_vertices = np.arange(path_ent.shape[0]-1)\n",
    "    target_vertices = [path_ent.shape[0]]\n",
    "    Q = harmonic_extension.compute_costs(L,source_vertices,target_vertices,source_embeddings.flatten(),target_embeddings.T,source_embeddings.shape[1])\n",
    "    ind = np.argpartition(Q, 10)[:10]\n",
    "    sorted_ind = ind[np.argsort(Q[ind])]\n",
    "    for kix in range(len(hits_at)):\n",
    "        if np.isin(path_ent[-1], sorted_ind[:hits_at[kix]]):\n",
    "            results[i,kix] = 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits@1: 3.6999999999999997%\n",
      "Hits@3: 6.1%\n",
      "Hits@5: 7.76%\n",
      "Hits@10: 10.26%\n"
     ]
    }
   ],
   "source": [
    "for kix in range(len(hits_at)):\n",
    "    print('Hits@{}: {}%'.format(hits_at[kix], np.sum(results[:,kix])/results.shape[0]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
